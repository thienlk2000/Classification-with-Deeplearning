{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e281e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\data\\\\animals'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6920b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6521a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8760ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'animals_data.py',\n",
       " 'raw-img',\n",
       " 'translate.py',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('D:/data/animals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c2ac40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cane',\n",
       " 'cavallo',\n",
       " 'elefante',\n",
       " 'farfalla',\n",
       " 'gallina',\n",
       " 'gatto',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'ragno',\n",
       " 'scoiattolo']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('D:/data/animals/raw-img/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca60209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = os.listdir('D:/data/animals/raw-img/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8802ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_name = {}\n",
    "name_to_idx = {}\n",
    "for idx, name in enumerate(class_name):\n",
    "    idx_to_name[idx] = name\n",
    "    name_to_idx[name] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a25e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'cane', 1: 'cavallo', 2: 'elefante', 3: 'farfalla', 4: 'gallina', 5: 'gatto', 6: 'mucca', 7: 'pecora', 8: 'ragno', 9: 'scoiattolo'}\n",
      "{'cane': 0, 'cavallo': 1, 'elefante': 2, 'farfalla': 3, 'gallina': 4, 'gatto': 5, 'mucca': 6, 'pecora': 7, 'ragno': 8, 'scoiattolo': 9}\n"
     ]
    }
   ],
   "source": [
    "print(idx_to_name)\n",
    "print(name_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a85e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob2\n",
    "img_dir = \"D:/data/animals/raw-img/\"\n",
    "img_file = glob2.glob(os.path.join(img_dir, '**','*.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23438645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24209"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c1d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "image_test = read_image(img_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e0f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[231, 231, 230,  ..., 240, 241, 241],\n",
      "         [231, 231, 230,  ..., 240, 240, 241],\n",
      "         [230, 230, 229,  ..., 239, 240, 240],\n",
      "         ...,\n",
      "         [ 86,  85,  84,  ..., 243, 243, 243],\n",
      "         [ 85,  85,  84,  ..., 243, 243, 243],\n",
      "         [ 93,  86,  81,  ..., 244, 244, 243]],\n",
      "\n",
      "        [[231, 231, 230,  ..., 233, 234, 234],\n",
      "         [231, 231, 230,  ..., 233, 233, 234],\n",
      "         [230, 230, 229,  ..., 232, 233, 233],\n",
      "         ...,\n",
      "         [ 88,  87,  86,  ..., 238, 238, 238],\n",
      "         [ 87,  87,  86,  ..., 238, 238, 238],\n",
      "         [ 95,  88,  83,  ..., 239, 239, 238]],\n",
      "\n",
      "        [[221, 221, 220,  ..., 227, 228, 228],\n",
      "         [221, 221, 220,  ..., 227, 227, 228],\n",
      "         [220, 220, 219,  ..., 226, 227, 227],\n",
      "         ...,\n",
      "         [ 87,  86,  85,  ..., 232, 232, 232],\n",
      "         [ 86,  86,  85,  ..., 232, 232, 232],\n",
      "         [ 94,  87,  82,  ..., 233, 233, 232]]], dtype=torch.uint8) torch.Size([3, 225, 300])\n"
     ]
    }
   ],
   "source": [
    "print(image_test, image_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e66c80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "img_test = Image.open(img_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2551cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=300x225 at 0x20290306630>\n"
     ]
    }
   ],
   "source": [
    "print(img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74b0d2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.JpegImagePlugin.JpegImageFile"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60012e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 225, 300])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToTensor()(img_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d28fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa9b698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "     ToTensor(),\n",
    "     Resize((224, 224)),\n",
    "     Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ee0ed67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.JpegImagePlugin.JpegImageFile"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "117f1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ToTensor()(img_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a70c38b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(img_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d16bf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/data/animals/raw-img\\\\cane\\\\OIF-e2bexWrojgtQnAPPcUfOWQ.jpeg'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a24893a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cane'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_file[0].split('\\\\')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d0821ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animals(Dataset):\n",
    "    def __init__(self, img_file, transform=None, label_transform=None):\n",
    "        self.img_file = img_file\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_file)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file = self.img_file[idx]\n",
    "        img = Image.open(file)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        name = file.split('\\\\')[-2]\n",
    "        idx = name_to_idx[name]\n",
    "        if self.label_transform is not None:\n",
    "            idx = self.label_transform(idx)\n",
    "        return img, idx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a4b560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Animals(img_file, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ba4547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, idx = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76d50d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b784b8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24209"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f6eca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bec228e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val = train_test_split(img_file, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b71a5493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19367, 4842)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8d7fc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/data/animals/raw-img\\\\gallina\\\\OIP-WTqPQAMuKTSXcNyvzQuUfQHaKO.jpeg',\n",
       " 'D:/data/animals/raw-img\\\\elefante\\\\OIP-pOQ3GJ5a0xfa6tex1hBbnwHaDr.jpeg',\n",
       " 'D:/data/animals/raw-img\\\\pecora\\\\OIP-0Yp_L1ZZITbuuSpCzv_R_wHaGQ.jpeg',\n",
       " 'D:/data/animals/raw-img\\\\ragno\\\\OIP-D60Jiw8svPHYQzBi6gtJBgHaE8.jpeg',\n",
       " 'D:/data/animals/raw-img\\\\ragno\\\\OIP-HHrSSeccaZG8529aj4vd2QHaGz.jpeg']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fee5b145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/data/animals/raw-img\\\\mucca\\\\OIP-rKxpte2TZpvUDiB18t8iLwHaEc.jpeg',\n",
       " 'D:/data/animals/raw-img\\\\elefante\\\\OIP-eB4vbil_6m5iZxj_YaztTQHaGM.jpeg',\n",
       " 'D:/data/animals/raw-img\\\\scoiattolo\\\\OIP-74sD_IjGYXucEibCgRwexAHaGL.jpeg',\n",
       " 'D:/data/animals/raw-img\\\\ragno\\\\OIP-Uh37C1ePBbQjKOW5cKWm_gHaLD.jpeg',\n",
       " 'D:/data/animals/raw-img\\\\gallina\\\\OIP-YvrKLNxUweIaelFgyreYcAHaEc.jpeg']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fcfd62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Animals(X_train, transform)\n",
    "val_set = Animals(X_val, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a68359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_load = DataLoader(val_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "412fefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "52845d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "da271861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9b802d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AlexNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(AlexNet).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 96, 11, 4, 0)\n",
    "#         self.relu1 = nn.ReLU\n",
    "#         self.maxpool1 = nn.MaxPool2d(3, 2)\n",
    "#         self.conv2 = nn.Conv2d(96, 256, 5, 1, 2)\n",
    "#         self.maxpool2 = nn.MaxPool2d(3, 2)\n",
    "#         self.conv3 = nn.\n",
    "\n",
    "alexnet = nn.Sequential(\n",
    "    nn.Conv2d(3, 96, 11, 4, 0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, 2),\n",
    "    nn.Conv2d(96, 256, 5, 1, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, 2),\n",
    "    nn.Conv2d(256, 384, 3, 1, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, 3, 1, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, 3, 1, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(6*6*256, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4096, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3c5ebab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "     ToTensor(),\n",
    "     Resize((227, 227)),\n",
    "     Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e5ad5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Animals(X_train, transform)\n",
    "val_set = Animals(X_val, transform)\n",
    "train_load = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_load = DataLoader(val_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "767597ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "feeda08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 227, 227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff10ceee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8fca7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(alexnet.parameters(), lr=1e-3, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a648a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU()\n",
       "  (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): ReLU()\n",
       "  (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU()\n",
       "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (13): Flatten(start_dim=1, end_dim=-1)\n",
       "  (14): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "  (15): ReLU()\n",
       "  (16): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (17): ReLU()\n",
       "  (18): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "alexnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a894de64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss is 2.3053276538848877\n",
      "iter 100 loss is 2.269758462905884\n",
      "iter 200 loss is 2.1956539154052734\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "n_sample = len(val_set)\n",
    "n_batch = len(val_load)\n",
    "for epoch in range(num_epochs):\n",
    "    alexnet.train()\n",
    "    for idx, (img, label) in enumerate(train_load):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        pred = alexnet(img)\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"iter {idx} loss is {loss}\")\n",
    "    alexnet.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in enumerate(val_load):\n",
    "            img - img.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = alexnet(img)\n",
    "            loss += loss_fn(pred, label)\n",
    "            val_acc += (pred == label).type(torch.float).sum().item()\n",
    "        val_loss = val_loss / n_batch\n",
    "        val_acc = val_acc / n_sample\n",
    "    print(f\"Epoch {epoch} Loss is {val_loss} Acc is {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8588e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
